{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##Imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport shutil\nfrom PIL import Image\n\n##Environment setup\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:17.417758Z","iopub.execute_input":"2024-06-21T13:59:17.418206Z","iopub.status.idle":"2024-06-21T13:59:31.566017Z","shell.execute_reply.started":"2024-06-21T13:59:17.418170Z","shell.execute_reply":"2024-06-21T13:59:31.564857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Problem Statement and Data Exploration\n\nThe task for this assignment it to implement style transfer using the CycleGAN architecture.\n\nStyle transfer is the process of apply the style of an artist to a given image. For this project, a set 7038 of photos and 300 Monet paintings are provided. The task is to apply Monet's signature impressionist style to mundane photos.\n\nBefore we begin setting up the model we start by looking at the dataset. The files are kept in a TF record format which necessitates a few loading functions to get the images out to feed to the model. Below are said function declarations followed with a count of the total images of each category and a sample of images from each. Additionally, the decode_image function handles what little preprocessing is necessary for the model. It normalizes the model to range from -1 to 1 and shapes it into an array the model can read.\n\nAdditionally, [guidance and code snippets have been provided by Amy Jang, who's notebook is referenced directly in the Kaggle challenge overview](https://www.kaggle.com/amyjang/monet-cyclegan-tutorial). Much of the loading and core aspects of the model (up and down sampling) have been pulled from her notebook with minimal change and added notation, but the model architecture has been modified for this assignment. Additionally, Ms Jang's notebook was somewhat out of data and called for using modules that no longer exist (the tensorflow addons module was deprecated earlier this year).","metadata":{}},{"cell_type":"code","source":"##Global declarations\nIMAGE_SIZE = [256, 256]\nGCS_PATH = KaggleDatasets().get_gcs_path()\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n\n##Function declarations\n##Reads in .tfrec files to images\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\n##Loads .tfrec images to a dataset\ndef load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset\n\n##Converts image to a float array. Assumes RGB format\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:31.568333Z","iopub.execute_input":"2024-06-21T13:59:31.569098Z","iopub.status.idle":"2024-06-21T13:59:32.499712Z","shell.execute_reply.started":"2024-06-21T13:59:31.569056Z","shell.execute_reply":"2024-06-21T13:59:32.498556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Output number of files\nprint('Monet TFRecord Files: %d'%len(MONET_FILENAMES))\nprint('Photo TFRecord Files: %d'%len(PHOTO_FILENAMES))\nprint(\"Number of Monet images: %d\"%len(os.listdir(\"/kaggle/input/gan-getting-started/monet_jpg\")))\nprint(\"Number of Photo images: %d\"%len(os.listdir(\"/kaggle/input/gan-getting-started/photo_jpg\")))\n\n##Print out samples of both photos and monet images\nmonet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)\n\nf = plt.figure(figsize=(15,5))\nfor idx,x in enumerate(iter(photo_ds)):\n    example_photo = x[0]\n    plt.subplot(2,5,idx+1)\n    plt.title('Photo %d'%(idx+1))\n    plt.imshow(example_photo * 0.5 + 0.5)\n    plt.xticks([])\n    plt.yticks([])\n    if idx>=4:\n        break\n\nfor idx,x in enumerate(iter(monet_ds)):\n    example_photo = x[0]\n    plt.subplot(2,5,idx+6)\n    plt.title('Photo %d'%(idx+1))\n    plt.imshow(example_photo * 0.5 + 0.5)\n    plt.xticks([])\n    plt.yticks([])\n    if idx>=4:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:32.506602Z","iopub.execute_input":"2024-06-21T13:59:32.507066Z","iopub.status.idle":"2024-06-21T13:59:35.450411Z","shell.execute_reply.started":"2024-06-21T13:59:32.507028Z","shell.execute_reply":"2024-06-21T13:59:35.448443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Setup\n\nThe CycleGAN architecture consists of two generators and two discriminators.\n\nEach of the generators is set up with an encoder that downsamples the emages repeatedly until it reaches an image embedding. The input images are same-padded and the convolutional layers use a stride of 2. Because of this, the image decreases by half in the X and Y dimensions on each downsample iteration. For this model, we use eight layers of downsampling to get to a 1x1x512 encoding for the image. \n\nOnce encoded, the generator then mirrors the process using transpose convolutional layers, which perform the convolution in reverse, enlarging the image on each step. Once more eight upsamples leads back to a 256x256x3 final image.","metadata":{}},{"cell_type":"code","source":"##Create the functions to up/down sample images\ndef downsample(filters, size, apply_instancenorm=True, batchSize=1):\n    #Initialize the 2D filter weights to ~N(0,.02), same for instance (group with axis=-1) normalization layer\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    #Create a 2D convolutional layer, instance normalization (as needed), and a leaky_relu\n    result = keras.Sequential()\n    result.add(keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n    if apply_instancenorm:\n        result.add(keras.layers.GroupNormalization(axis=-1,gamma_initializer=gamma_init))\n    result.add(keras.layers.LeakyReLU())\n\n    return result\n\n#Downsample, but in reverse\ndef upsample(filters, size, apply_dropout=False):\n    #Initialize weights to ~N(0,.02), same for instance (group with axis=-1) normalization layer\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    #Create the transpose convolutional layer to build new data, instance normalization, dropout (as needed) and a relu\n    result = keras.Sequential()\n    result.add(keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n    result.add(keras.layers.GroupNormalization(axis=-1,gamma_initializer=gamma_init))\n    if apply_dropout:\n        result.add(keras.layers.Dropout(0.5))\n    result.add(keras.layers.ReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:35.451731Z","iopub.execute_input":"2024-06-21T13:59:35.452045Z","iopub.status.idle":"2024-06-21T13:59:35.461970Z","shell.execute_reply.started":"2024-06-21T13:59:35.452019Z","shell.execute_reply":"2024-06-21T13:59:35.460887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the up and down sampling layers have been defined, it's time to arrange them in the model.\n\nThe generator, as noted before, consists of a stack of downsampling layers. Between them are 5 residual blocks, which are comprised of 3x3 convolutional layers that are same padded and use stride 1. This architecture differs from the original tutorial notebook in that it does not go down to a 1x1x512, but instead stays at 32x32xN and is passed through the aforementioned residual layers.","metadata":{}},{"cell_type":"code","source":"def Generator():\n    #Define initialization\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inputs = keras.layers.Input(shape=[256,256,3])\n\n    #Create the stack of downsampling layers\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n    ]\n    #Create the stack of upsampling layers\n    up_stack = [\n        upsample(512, 4), # (bs, 2, 2, 1024)\n        upsample(512, 4), # (bs, 4, 4, 1024)\n        upsample(512, 4), # (bs, 8, 8, 1024)\n    ]\n    #Define the output layer\n    last = keras.layers.Conv2DTranspose(3, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n    \n    #Stitch the model together\n    x = inputs\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    skips = reversed(skips[:-1])\n    \n    for idx in range(5):\n        keras.layers.Conv2D(256,3)(x)\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = keras.layers.Concatenate()([x, skip])\n    #Staple the output layer to the model\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:35.463447Z","iopub.execute_input":"2024-06-21T13:59:35.463773Z","iopub.status.idle":"2024-06-21T13:59:35.479801Z","shell.execute_reply.started":"2024-06-21T13:59:35.463747Z","shell.execute_reply":"2024-06-21T13:59:35.478604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The discriminators are much simpler in comparison. Images are downsampled to end up at 30x30 and evaluated against whether or not an image is a fake.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = keras.layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n    #Set up downsample stack\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n    zero_pad1 = keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = keras.layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n    norm1 = keras.layers.Normalization()(conv)\n    leaky_relu = keras.layers.LeakyReLU()(norm1)\n    zero_pad2 = keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n    last = keras.layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:35.481616Z","iopub.execute_input":"2024-06-21T13:59:35.481977Z","iopub.status.idle":"2024-06-21T13:59:35.496021Z","shell.execute_reply.started":"2024-06-21T13:59:35.481948Z","shell.execute_reply":"2024-06-21T13:59:35.494701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, the fully built model. When called, users must provide the aforementioned 2 generators and descriminators. When the model is compiled, users will also need to provide optimizers and loss functions.\n\nThe model itself needs further explanation and is poorly served only in text. On each cycle a few steps are made.\n\nFirst an array of images are made.\n* A photo is passed through the monet generator, and fed back through the photo generator to evaluate how well it can recreate the same image.\n* The above step is applied to monet images.\n* A Monet is fed to the Monet generator and a photo to the photo generator.\n\nThese generated and real photos are fed to the discriminators to get those results.\n\nThe discriminator results are then used to create losses for:\n* Both discriminators\n* Both generators\n* Cycle consistancy loss. The loss of an image being put through one generator and back through the other.\n\nThese losses are backpropgated through the appropriate model and the cycle continues.","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients, self.m_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients, self.p_gen.trainable_variables))\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients, self.m_disc.trainable_variables))\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients, self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:35.497742Z","iopub.execute_input":"2024-06-21T13:59:35.498091Z","iopub.status.idle":"2024-06-21T13:59:35.550434Z","shell.execute_reply.started":"2024-06-21T13:59:35.498064Z","shell.execute_reply":"2024-06-21T13:59:35.549362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we define the various losses used by the model. The discriminators use binary cross-entropy, as one would expect given the output is a logical defining a given image as true or false. The loss is aggregated for both the real and fake images into a total discriminator loss. The discriminator results are used to create then create the generator loss. The cycle consistancy loss is created by comparing the original image to the one that was passed through both generators and the identity loss is created from each generator being fed an image of that category.\n\nThe discriminator losses are averaged between the real and fake examples, the generator losses are taken directly, the cycle consistancy loss and identity loss are both mean absolute error.\n\nFinally, an ADAM optimizer is used for each of the models. Once everything is defined, the final aggregate model is compiled and ready to train.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    #Define the various losses for the model to use\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n        total_disc_loss = real_loss + generated_loss\n        return total_disc_loss * 0.5\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n        return LAMBDA * loss1\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss\n    #Specify the optimizer functions\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    #Initialize the generators/discriminators\n    monet_generator = Generator() # transforms photos to Monet-esque paintings\n    photo_generator = Generator() # transforms Monet paintings to be more like photos\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos\n    #Build and compile the model\n    cycle_gan_model = CycleGan(\n            monet_generator, photo_generator, monet_discriminator, photo_discriminator\n        )\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:35.551836Z","iopub.execute_input":"2024-06-21T13:59:35.552153Z","iopub.status.idle":"2024-06-21T13:59:36.296709Z","shell.execute_reply.started":"2024-06-21T13:59:35.552126Z","shell.execute_reply":"2024-06-21T13:59:36.295381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_ds, photo_ds)),\n    epochs=100,\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:36.299623Z","iopub.execute_input":"2024-06-21T13:59:36.300021Z","iopub.status.idle":"2024-06-21T13:59:36.304584Z","shell.execute_reply.started":"2024-06-21T13:59:36.299992Z","shell.execute_reply":"2024-06-21T13:59:36.303440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nBelow shows a sample output of Monet transferred images. The first image shows it somewhat working. The impressionistic style is there, but doesn't show up terribly well on images such as the drone. You can still tell that the images are softer and at least attempt to be impressionistic. An ok first attempt at style transfer.\n\nMuch of the ineffectiveness of teh model is likely due to using an older architecture that doesn't go down to a 1x1x512 feature layer and simply isn't as good as newer versions. Due to relying on the tutorial, I was hesitant to do so and reverted to implementing what I believe is an older version of CycleGAN. I also noticed the issue with square structures in the images that GANs sometimes make. I know there is a technique to address these, but I cannot locate it at this time. Researching that and implementing would also aid the model. There's also the argument of slowing down the learning rate on the discriminators as an experiment. Should they learn too quickly, then the model will stop learning.\n\nIn all, the model proves the possibility of using CycleGAN for style transfer, but doesn't quite succeed. Further iterations would be necessary to have the model working to the desired level.","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:59:36.306112Z","iopub.execute_input":"2024-06-21T13:59:36.306461Z","iopub.status.idle":"2024-06-21T13:59:41.910375Z","shell.execute_reply.started":"2024-06-21T13:59:36.306433Z","shell.execute_reply":"2024-06-21T13:59:41.909185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Generate kaggle submission\nsavePath = \"/kaggle/images/\"\nif not os.path.exists(savePath):\n    os.mkdir(savePath)\nfor idx,img in enumerate(photo_ds.take(7000)):\n    pred = monet_generator(img,training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n    im = Image.fromarray(img)\n    im.save(savePath+\"p_%d.jpg\"%idx)\nshutil.make_archive(\"/kaggle/working/images\",'zip',\"/kaggle/images/\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T14:05:18.620373Z","iopub.execute_input":"2024-06-21T14:05:18.620768Z","iopub.status.idle":"2024-06-21T14:05:26.613814Z","shell.execute_reply.started":"2024-06-21T14:05:18.620737Z","shell.execute_reply":"2024-06-21T14:05:26.612717Z"},"trusted":true},"execution_count":null,"outputs":[]}]}